import json
from typing import Any, Dict, List, Optional

from autogen_core.base import AgentProxy, CancellationToken, MessageContext, TopicId
from autogen_core.components import default_subscription
from autogen_core.components.models import (
    AssistantMessage,
    ChatCompletionClient,
    LLMMessage,
    SystemMessage,
    UserMessage,
)

from ..messages import BroadcastMessage, OrchestrationEvent, ResetMessage
from .base_orchestrator import BaseOrchestrator
from .orchestrator_prompts import (
    ORCHESTRATOR_CLOSED_BOOK_PROMPT,
    ORCHESTRATOR_GET_FINAL_ANSWER,
    ORCHESTRATOR_LEDGER_PROMPT,
    ORCHESTRATOR_PLAN_PROMPT,
    ORCHESTRATOR_SYNTHESIZE_PROMPT,
    ORCHESTRATOR_SYSTEM_MESSAGE,
    ORCHESTRATOR_UPDATE_FACTS_PROMPT,
    ORCHESTRATOR_UPDATE_PLAN_PROMPT,
)



@default_subscription
class LedgerOrchestrator(BaseOrchestrator):
    """The LedgerOrhestrator is the orchestrator used by MagenticOne to solve tasks.
    It uses a ledger (implemented as a JSON generated by the LLM) to keep track of task progress and select the next agent that should speak."""

    DEFAULT_SYSTEM_MESSAGES = [
        SystemMessage(ORCHESTRATOR_SYSTEM_MESSAGE),
    ]

    def __init__(
        self,
        agents: List[AgentProxy],
        model_client: ChatCompletionClient,
        description: str = "Ledger-based orchestrator",
        system_messages: List[SystemMessage] = DEFAULT_SYSTEM_MESSAGES,
        closed_book_prompt: str = ORCHESTRATOR_CLOSED_BOOK_PROMPT,
        plan_prompt: str = ORCHESTRATOR_PLAN_PROMPT,
        synthesize_prompt: str = ORCHESTRATOR_SYNTHESIZE_PROMPT,
        ledger_prompt: str = ORCHESTRATOR_LEDGER_PROMPT,
        update_facts_prompt: str = ORCHESTRATOR_UPDATE_FACTS_PROMPT,
        update_plan_prompt: str = ORCHESTRATOR_UPDATE_PLAN_PROMPT,
        max_rounds: int = 20,
        max_time: float = float("inf"),
        max_stalls_before_replan: int = 3,
        max_replans: int = 3,
        return_final_answer: bool = False,
    ) -> None:
        super().__init__(agents=agents, description=description, max_rounds=max_rounds, max_time=max_time)

        self._model_client = model_client

        # prompt-based parameters
        self._system_messages = system_messages
        self._closed_book_prompt = closed_book_prompt
        self._plan_prompt = plan_prompt
        self._synthesize_prompt = synthesize_prompt
        self._ledger_prompt = ledger_prompt
        self._update_facts_prompt = update_facts_prompt
        self._update_plan_prompt = update_plan_prompt

        self._chat_history: List[LLMMessage] = []
        self._should_replan = True
        self._max_stalls_before_replan = max_stalls_before_replan
        self._stall_counter = 0
        self._max_replans = max_replans
        self._replan_counter = 0
        self._return_final_answer = return_final_answer

        self._team_description = ""
        self._task = ""
        self._facts = ""
        self._plan = ""

    def _get_closed_book_prompt(self, task: str) -> str:
        return self._closed_book_prompt.format(task=task)

    def _get_plan_prompt(self, team: str) -> str:
        return self._plan_prompt.format(team=team)

    def _get_synthesize_prompt(self, task: str, team: str, facts: str, plan: str) -> str:
        return self._synthesize_prompt.format(task=task, team=team, facts=facts, plan=plan)

    def _get_ledger_prompt(self, task: str, team: str, names: List[str]) -> str:
        return self._ledger_prompt.format(task=task, team=team, names=names)

    def _get_update_facts_prompt(self, task: str, facts: str) -> str:
        return self._update_facts_prompt.format(task=task, facts=facts)

    def _get_update_plan_prompt(self, team: str) -> str:
        return self._update_plan_prompt.format(team=team)

    async def _get_team_description(self) -> str:
        # a single string description of all agents in the team
        team_description = ""
        for agent in self._agents:
            metadata = await agent.metadata
            name = metadata["type"]
            description = metadata["description"]
            team_description += f"{name}: {description}\n"
        return team_description

    async def _get_team_names(self) -> List[str]:
        return [(await agent.metadata)["type"] for agent in self._agents]

    def _get_message_str(self, message: LLMMessage) -> str: #TODO: implementz
        if isinstance(message.content, str):
            return message.content
        else:
            result = ""
            for content in message.content:
                if isinstance(content, str):
                    result += content + "\n"
            assert len(result) > 0
        return result

    async def _initialize_task(self, task: str, cancellation_token: Optional[CancellationToken] = None) -> None:
        # called the first time a task is received
        self._task = task
        self._team_description = await self._get_team_description()

        # Shallow-copy the conversation
        planning_conversation = [m for m in self._chat_history]

        # 1. GATHER FACTS
        # create a closed book task and generate a response and update the chat history
        planning_conversation.append(
            UserMessage(content=self._get_closed_book_prompt(self._task), source=self.metadata["type"])
        )
        response = await self._model_client.create(
            self._system_messages + planning_conversation, cancellation_token=cancellation_token
        )

        assert isinstance(response.content, str)
        self._facts = response.content
        planning_conversation.append(AssistantMessage(content=self._facts, source=self.metadata["type"]))

        # 2. CREATE A PLAN
        ## plan based on available information
        planning_conversation.append(
            UserMessage(content=self._get_plan_prompt(self._team_description), source=self.metadata["type"])
        )
        response = await self._model_client.create(
            self._system_messages + planning_conversation, cancellation_token=cancellation_token
        )

        assert isinstance(response.content, str)
        self._plan = response.content

        # At this point, the planning conversation is dropped.

    async def _update_facts_and_plan(self, cancellation_token: Optional[CancellationToken] = None) -> None:
        # called when the orchestrator decides to replan

        # Shallow-copy the conversation
        planning_conversation = [m for m in self._chat_history]

        # Update the facts
        planning_conversation.append(
            UserMessage(content=self._get_update_facts_prompt(self._task, self._facts), source=self.metadata["type"])
        )
        response = await self._model_client.create(
            self._system_messages + planning_conversation, cancellation_token=cancellation_token
        )

        assert isinstance(response.content, str)
        self._facts = response.content
        planning_conversation.append(AssistantMessage(content=self._facts, source=self.metadata["type"]))

        # Update the plan
        planning_conversation.append(
            UserMessage(content=self._get_update_plan_prompt(self._team_description), source=self.metadata["type"])
        )
        response = await self._model_client.create(
            self._system_messages + planning_conversation, cancellation_token=cancellation_token
        )

        assert isinstance(response.content, str)
        self._plan = response.content

    async def update_ledger(self, cancellation_token: Optional[CancellationToken] = None) -> Dict[str, Any]:
        # updates the ledger at each turn
        max_json_retries = 10

        team_description = await self._get_team_description()
        names = await self._get_team_names()
        ledger_prompt = self._get_ledger_prompt(self._task, team_description, names)

        ledger_user_messages: List[LLMMessage] = [UserMessage(content=ledger_prompt, source=self.metadata["type"])]

        # retries in case the LLM does not return a valid JSON
        assert max_json_retries > 0
        for _ in range(max_json_retries):
            ledger_response = await self._model_client.create(
                self._system_messages + self._chat_history + ledger_user_messages,
                json_output=True,
                cancellation_token=cancellation_token,
            )
            ledger_str = ledger_response.content

            try:
                assert isinstance(ledger_str, str)
                ledger_dict: Dict[str, Any] = json.loads(ledger_str)
                required_keys = [
                    "is_request_satisfied",
                    "is_in_loop",
                    "is_progress_being_made",
                    "next_speaker",
                    "instruction_or_question",
                ]
                key_error = False
                for key in required_keys:
                    if key not in ledger_dict:
                        ledger_user_messages.append(AssistantMessage(content=ledger_str, source="self"))
                        ledger_user_messages.append(
                            UserMessage(content=f"KeyError: '{key}'", source=self.metadata["type"])
                        )
                        key_error = True
                        break
                    if "answer" not in ledger_dict[key]:
                        ledger_user_messages.append(AssistantMessage(content=ledger_str, source="self"))
                        ledger_user_messages.append(
                            UserMessage(content=f"KeyError: '{key}.answer'", source=self.metadata["type"])
                        )
                        key_error = True
                        break
                if key_error:
                    continue
                return ledger_dict
            except json.JSONDecodeError as e:

                self.logger.error("An error occurred in update_ledger: %s", traceback.format_exc())

                self.logger.info(
                    OrchestrationEvent(
                        f"{self.metadata['type']} (error)",
                        f"Failed to parse ledger information: {ledger_str}",
                    )
                )
                raise e

        raise ValueError("Failed to parse ledger information after multiple retries.")

    async def _prepare_final_answer(self, cancellation_token: Optional[CancellationToken] = None) -> str:
        # called when the task is complete

        final_message = UserMessage(
            content=ORCHESTRATOR_GET_FINAL_ANSWER.format(task=self._task), source=self.metadata["type"]
        )
        response = await self._model_client.create(
            self._system_messages + self._chat_history + [final_message], cancellation_token=cancellation_token
        )

        assert isinstance(response.content, str)

        return response.content

    async def _handle_broadcast(self, message: BroadcastMessage, ctx: MessageContext) -> None:
        self._chat_history.append(message.content)
        await super()._handle_broadcast(message, ctx)

    async def _select_next_agent(
        self, message: LLMMessage, cancellation_token: Optional[CancellationToken] = None
    ) -> Optional[AgentProxy]:
        # the main orchestrator loop
        # Check if the task is still unset, in which case this message contains the task string
        if len(self._task) == 0:
            await self._initialize_task(self._get_message_str(message), cancellation_token)

            # At this point the task, plan and facts shouls all be set
            assert len(self._task) > 0
            assert len(self._facts) > 0
            assert len(self._plan) > 0
            assert len(self._team_description) > 0

            # Send everyone the plan
            synthesized_prompt = self._get_synthesize_prompt(
                self._task, self._team_description, self._facts, self._plan
            )
            topic_id = TopicId("default", self.id.key)
            await self.publish_message(
                BroadcastMessage(content=UserMessage(content=synthesized_prompt, source=self.metadata["type"])),
                topic_id=topic_id,
                cancellation_token=cancellation_token,
            )

            self.logger.info(
                OrchestrationEvent(
                    f"{self.metadata['type']} (thought)",
                    f"Initial plan:\n{synthesized_prompt}",
                )
            )

            self._replan_counter = 0
            self._stall_counter = 0

            synthesized_message = AssistantMessage(content=synthesized_prompt, source=self.metadata["type"])
            self._chat_history.append(synthesized_message)

            # Answer from this synthesized message
            return await self._select_next_agent(synthesized_message, cancellation_token)

        # Orchestrate the next step
        ledger_dict = await self.update_ledger(cancellation_token)
        self.logger.info(
            OrchestrationEvent(
                f"{self.metadata['type']} (thought)",
                f"Updated Ledger:\n{json.dumps(ledger_dict, indent=2)}",
            )
        )

        # Task is complete
        if ledger_dict["is_request_satisfied"]["answer"] is True:
            self.logger.info(
                OrchestrationEvent(
                    f"{self.metadata['type']} (thought)",
                    "Request satisfied.",
                )
            )
            if self._return_final_answer:
                # generate a final message to summarize the conversation
                final_answer = await self._prepare_final_answer(cancellation_token)
                self.logger.info(
                    OrchestrationEvent(
                        f"{self.metadata['type']} (final answer)",
                        f"\n{final_answer}",
                    )
                )
            return None

        # Stalled or stuck in a loop
        stalled = ledger_dict["is_in_loop"]["answer"] or not ledger_dict["is_progress_being_made"]["answer"]
        if stalled:
            self._stall_counter += 1

            # We exceeded our stall counter, so we need to replan, or exit
            if self._stall_counter > self._max_stalls_before_replan:
                self._replan_counter += 1
                self._stall_counter = 0

                # We exceeded our replan counter
                if self._replan_counter > self._max_replans:
                    self.logger.info(
                        OrchestrationEvent(
                            f"{self.metadata['type']} (thought)",
                            "Replan counter exceeded... Terminating.",
                        )
                    )
                    return None
                # Let's create a new plan
                else:
                    self.logger.info(
                        OrchestrationEvent(
                            f"{self.metadata['type']} (thought)",
                            "Stalled.... Replanning...",
                        )
                    )

                    # Update our plan.
                    await self._update_facts_and_plan(cancellation_token)

                    # Reset everyone, then rebroadcast the new plan
                    self._chat_history = [self._chat_history[0]]
                    topic_id = TopicId("default", self.id.key)
                    await self.publish_message(ResetMessage(), topic_id=topic_id, cancellation_token=cancellation_token)

                    # Send everyone the NEW plan
                    synthesized_prompt = self._get_synthesize_prompt(
                        self._task, self._team_description, self._facts, self._plan
                    )
                    await self.publish_message(
                        BroadcastMessage(content=UserMessage(content=synthesized_prompt, source=self.metadata["type"])),
                        topic_id=topic_id,
                        cancellation_token=cancellation_token,
                    )

                    self.logger.info(
                        OrchestrationEvent(
                            f"{self.metadata['type']} (thought)",
                            f"New plan:\n{synthesized_prompt}",
                        )
                    )

                    synthesized_message = AssistantMessage(content=synthesized_prompt, source=self.metadata["type"])
                    self._chat_history.append(synthesized_message)

                    # Answer from this synthesized message
                    return await self._select_next_agent(synthesized_message, cancellation_token)

        # If we goit this far, we were not starting, done, or stuck
        next_agent_name = ledger_dict["next_speaker"]["answer"]
        # find the agent with the next agent name
        for agent in self._agents:
            if (await agent.metadata)["type"] == next_agent_name:
                # broadcast a new message
                instruction = ledger_dict["instruction_or_question"]["answer"]
                user_message = UserMessage(content=instruction, source=self.metadata["type"])
                assistant_message = AssistantMessage(content=instruction, source=self.metadata["type"])
                self.logger.info(OrchestrationEvent(f"{self.metadata['type']} (-> {next_agent_name})", instruction))
                self._chat_history.append(assistant_message)  # My copy
                topic_id = TopicId("default", self.id.key)
                await self.publish_message(
                    BroadcastMessage(content=user_message, request_halt=False),
                    topic_id=topic_id,
                    cancellation_token=cancellation_token,
                )  # Send to everyone else
                return agent

        return None


class BaseOrchestrator(MagenticOneBaseAgent):
    """Base class for orchestrator that manage a group of agents."""

    def __init__(
        self,
        agents: List[AgentProxy],
        description: str = "Base orchestrator",
        max_rounds: int = 20,
        max_time: float = float("inf"),
        handle_messages_concurrently: bool = False,
    ) -> None:
        super().__init__(description, handle_messages_concurrently=handle_messages_concurrently)
        self._agents = agents
        self._max_rounds = max_rounds
        self._max_time = max_time
        self._num_rounds = 0
        self._start_time: float = -1.0
        self.logger = logging.getLogger(EVENT_LOGGER_NAME + f".{self.id.key}.orchestrator")

    async def _handle_broadcast(self, message: BroadcastMessage, ctx: MessageContext) -> None:
        """Handle an incoming message."""

        # First broadcast sets the timer
        if self._start_time < 0:
            self._start_time = time.time()

        source = "Unknown"
        if isinstance(message.content, UserMessage) or isinstance(message.content, AssistantMessage):
            source = message.content.source

        content = message_content_to_str(message.content.content)

        self.logger.info(OrchestrationEvent(source, content))

        # Termination conditions
        if self._num_rounds >= self._max_rounds:
            self.logger.info(
                OrchestrationEvent(
                    f"{self.metadata['type']} (termination condition)",
                    f"Max rounds ({self._max_rounds}) reached.",
                )
            )
            return

        if time.time() - self._start_time >= self._max_time:
            self.logger.info(
                OrchestrationEvent(
                    f"{self.metadata['type']} (termination condition)",
                    f"Max time ({self._max_time}s) reached.",
                )
            )
            return

        if message.request_halt:
            self.logger.info(
                OrchestrationEvent(
                    f"{self.metadata['type']} (termination condition)",
                    f"{source} requested halt.",
                )
            )
            return

        next_agent = await self._select_next_agent(message.content)
        if next_agent is None:
            self.logger.info(
                OrchestrationEvent(
                    f"{self.metadata['type']} (termination condition)",
                    "No agent selected.",
                )
            )
            return
        request_reply_message = RequestReplyMessage()
        # emit an event

        self.logger.info(
            OrchestrationEvent(
                source=f"{self.metadata['type']} (thought)",
                message=f"Next speaker {(await next_agent.metadata)['type']}" "",
            )
        )

        self._num_rounds += 1  # Call before sending the message
        await self.send_message(request_reply_message, next_agent.id, cancellation_token=ctx.cancellation_token)

    async def _select_next_agent(self, message: LLMMessage) -> Optional[AgentProxy]:
        raise NotImplementedError()

    def get_max_rounds(self) -> int:
        return self._max_rounds

    async def _handle_reset(self, message: ResetMessage, ctx: MessageContext) -> None:
        """Handle a reset message."""
        await self._reset(ctx.cancellation_token)

    async def _reset(self, cancellation_token: CancellationToken) -> None:
        pass


class MagenticOneBaseAgent(RoutedAgent):
    """An agent that optionally ensures messages are handled non-concurrently in the order they arrive."""

    def __init__(
        self,
        description: str,
        handle_messages_concurrently: bool = False,
    ) -> None:
        super().__init__(description)
        self._handle_messages_concurrently = handle_messages_concurrently
        self._enabled = True
        self.logger = logging.getLogger(EVENT_LOGGER_NAME + f".{self.id.key}.agent")

        if not self._handle_messages_concurrently:
            # TODO: make it possible to stop
            self._message_queue = asyncio.Queue[tuple[MagenticOneMessages, MessageContext, asyncio.Future[Any]]]()
            self._processing_task = asyncio.create_task(self._process())

    async def _process(self) -> None:
        while True:
            message, ctx, future = await self._message_queue.get()
            if ctx.cancellation_token.is_cancelled():
                # TODO: Do we need to resolve the future here?
                future.cancel()
                continue

            try:
                if isinstance(message, RequestReplyMessage):
                    await self._handle_request_reply(message, ctx)
                elif isinstance(message, BroadcastMessage):
                    await self._handle_broadcast(message, ctx)
                elif isinstance(message, ResetMessage):
                    await self._handle_reset(message, ctx)
                elif isinstance(message, DeactivateMessage):
                    await self._handle_deactivate(message, ctx)
                else:
                    raise ValueError("Unknown message type.")
                future.set_result(None)
            except asyncio.CancelledError:
                future.cancel()
            except Exception as e:
                future.set_exception(e)

    @message_handler
    async def handle_incoming_message(
        self,
        message: BroadcastMessage | ResetMessage | DeactivateMessage | RequestReplyMessage,
        ctx: MessageContext,
    ) -> None:
        if not self._enabled:
            return

        if self._handle_messages_concurrently:
            if isinstance(message, RequestReplyMessage):
                await self._handle_request_reply(message, ctx)
            elif isinstance(message, BroadcastMessage):
                await self._handle_broadcast(message, ctx)
            elif isinstance(message, ResetMessage):
                await self._handle_reset(message, ctx)
            elif isinstance(message, DeactivateMessage):
                await self._handle_deactivate(message, ctx)
        else:
            future = asyncio.Future[Any]()
            await self._message_queue.put((message, ctx, future))
            await future

    async def _handle_broadcast(self, message: BroadcastMessage, ctx: MessageContext) -> None:
        raise NotImplementedError()

    async def _handle_reset(self, message: ResetMessage, ctx: MessageContext) -> None:
        raise NotImplementedError()

    async def _handle_request_reply(self, message: RequestReplyMessage, ctx: MessageContext) -> None:
        raise NotImplementedError()

    async def _handle_deactivate(self, message: DeactivateMessage, ctx: MessageContext) -> None:
        """Handle a deactivate message."""
        self._enabled = False
        self.logger.info(
            AgentEvent(
                f"{self.metadata['type']} (deactivated)",
                "",
            )
        )

    async def on_unhandled_message(self, message: Any, ctx: MessageContext) -> None:
        """Drop the message, with a log."""
        # self.logger.info(
        #     AgentEvent(
        #         f"{self.metadata['type']} (unhandled message)",
        #         f"Unhandled message type: {type(message)}",
        #     )
        # )
        pass


from typing import List, Tuple

from autogen_core.base import CancellationToken, MessageContext, TopicId
from autogen_core.components.models import (
    AssistantMessage,
    LLMMessage,
    UserMessage,
)

from autogen_magentic_one.messages import (
    BroadcastMessage,
    RequestReplyMessage,
    ResetMessage,
    UserContent,
)

from ..utils import message_content_to_str
from .base_agent import MagenticOneBaseAgent


class BaseWorker(MagenticOneBaseAgent):
    """Base agent that handles the MagenticOne worker behavior protocol."""

    def __init__(
        self,
        description: str,
        handle_messages_concurrently: bool = False,
    ) -> None:
        super().__init__(description, handle_messages_concurrently=handle_messages_concurrently)
        self._chat_history: List[LLMMessage] = []

    async def _handle_broadcast(self, message: BroadcastMessage, ctx: MessageContext) -> None:
        assert isinstance(message.content, UserMessage)
        self._chat_history.append(message.content)

    async def _handle_reset(self, message: ResetMessage, ctx: MessageContext) -> None:
        """Handle a reset message."""
        await self._reset(ctx.cancellation_token)

    async def _handle_request_reply(self, message: RequestReplyMessage, ctx: MessageContext) -> None:
        """Respond to a reply request."""
        request_halt, response = await self._generate_reply(ctx.cancellation_token)

        assistant_message = AssistantMessage(content=message_content_to_str(response), source=self.metadata["type"])
        self._chat_history.append(assistant_message)

        user_message = UserMessage(content=response, source=self.metadata["type"])
        topic_id = TopicId("default", self.id.key)
        await self.publish_message(
            BroadcastMessage(content=user_message, request_halt=request_halt),
            topic_id=topic_id,
            cancellation_token=ctx.cancellation_token,
        )

    async def _generate_reply(self, cancellation_token: CancellationToken) -> Tuple[bool, UserContent]:
        """Returns (request_halt, response_message)"""
        raise NotImplementedError()

    async def _reset(self, cancellation_token: CancellationToken) -> None:
        self._chat_history = []


import logging
import time
from typing import List, Optional

from autogen_core.application.logging import EVENT_LOGGER_NAME
from autogen_core.base import AgentProxy, CancellationToken, MessageContext
from autogen_core.components.models import AssistantMessage, LLMMessage, UserMessage

from ..messages import BroadcastMessage, OrchestrationEvent, RequestReplyMessage, ResetMessage
from ..utils import message_content_to_str
from .base_agent import MagenticOneBaseAgent


class BaseOrchestrator(MagenticOneBaseAgent):
    """Base class for orchestrator that manage a group of agents."""

    def __init__(
        self,
        agents: List[AgentProxy],
        description: str = "Base orchestrator",
        max_rounds: int = 20,
        max_time: float = float("inf"),
        handle_messages_concurrently: bool = False,
    ) -> None:
        super().__init__(description, handle_messages_concurrently=handle_messages_concurrently)
        self._agents = agents
        self._max_rounds = max_rounds
        self._max_time = max_time
        self._num_rounds = 0
        self._start_time: float = -1.0
        self.logger = logging.getLogger(EVENT_LOGGER_NAME + f".{self.id.key}.orchestrator")

    async def _handle_broadcast(self, message: BroadcastMessage, ctx: MessageContext) -> None:
        """Handle an incoming message."""

        # First broadcast sets the timer
        if self._start_time < 0:
            self._start_time = time.time()

        source = "Unknown"
        if isinstance(message.content, UserMessage) or isinstance(message.content, AssistantMessage):
            source = message.content.source

        content = message_content_to_str(message.content.content)

        self.logger.info(OrchestrationEvent(source, content))

        # Termination conditions
        if self._num_rounds >= self._max_rounds:
            self.logger.info(
                OrchestrationEvent(
                    f"{self.metadata['type']} (termination condition)",
                    f"Max rounds ({self._max_rounds}) reached.",
                )
            )
            return

        if time.time() - self._start_time >= self._max_time:
            self.logger.info(
                OrchestrationEvent(
                    f"{self.metadata['type']} (termination condition)",
                    f"Max time ({self._max_time}s) reached.",
                )
            )
            return

        if message.request_halt:
            self.logger.info(
                OrchestrationEvent(
                    f"{self.metadata['type']} (termination condition)",
                    f"{source} requested halt.",
                )
            )
            return

        next_agent = await self._select_next_agent(message.content)
        if next_agent is None:
            self.logger.info(
                OrchestrationEvent(
                    f"{self.metadata['type']} (termination condition)",
                    "No agent selected.",
                )
            )
            return
        request_reply_message = RequestReplyMessage()
        # emit an event

        self.logger.info(
            OrchestrationEvent(
                source=f"{self.metadata['type']} (thought)",
                message=f"Next speaker {(await next_agent.metadata)['type']}" "",
            )
        )

        self._num_rounds += 1  # Call before sending the message
        await self.send_message(request_reply_message, next_agent.id, cancellation_token=ctx.cancellation_token)

    async def _select_next_agent(self, message: LLMMessage) -> Optional[AgentProxy]:
        raise NotImplementedError()

    def get_max_rounds(self) -> int:
        return self._max_rounds

    async def _handle_reset(self, message: ResetMessage, ctx: MessageContext) -> None:
        """Handle a reset message."""
        await self._reset(ctx.cancellation_token)

    async def _reset(self, cancellation_token: CancellationToken) -> None:
        pass

import asyncio
import logging
from typing import Any

from autogen_core.application.logging import EVENT_LOGGER_NAME
from autogen_core.base import MessageContext
from autogen_core.components import RoutedAgent, message_handler

from autogen_magentic_one.messages import (
    AgentEvent,
    BroadcastMessage,
    DeactivateMessage,
    MagenticOneMessages,
    RequestReplyMessage,
    ResetMessage,
)


class MagenticOneBaseAgent(RoutedAgent):
    """An agent that optionally ensures messages are handled non-concurrently in the order they arrive."""

    def __init__(
        self,
        description: str,
        handle_messages_concurrently: bool = False,
    ) -> None:
        super().__init__(description)
        self._handle_messages_concurrently = handle_messages_concurrently
        self._enabled = True
        self.logger = logging.getLogger(EVENT_LOGGER_NAME + f".{self.id.key}.agent")

        if not self._handle_messages_concurrently:
            # TODO: make it possible to stop
            self._message_queue = asyncio.Queue[tuple[MagenticOneMessages, MessageContext, asyncio.Future[Any]]]()
            self._processing_task = asyncio.create_task(self._process())

    async def _process(self) -> None:
        while True:
            message, ctx, future = await self._message_queue.get()
            if ctx.cancellation_token.is_cancelled():
                # TODO: Do we need to resolve the future here?
                future.cancel()
                continue

            try:
                if isinstance(message, RequestReplyMessage):
                    await self._handle_request_reply(message, ctx)
                elif isinstance(message, BroadcastMessage):
                    await self._handle_broadcast(message, ctx)
                elif isinstance(message, ResetMessage):
                    await self._handle_reset(message, ctx)
                elif isinstance(message, DeactivateMessage):
                    await self._handle_deactivate(message, ctx)
                else:
                    raise ValueError("Unknown message type.")
                future.set_result(None)
            except asyncio.CancelledError:
                future.cancel()
            except Exception as e:
                future.set_exception(e)

    @message_handler
    async def handle_incoming_message(
        self,
        message: BroadcastMessage | ResetMessage | DeactivateMessage | RequestReplyMessage,
        ctx: MessageContext,
    ) -> None:
        if not self._enabled:
            return

        if self._handle_messages_concurrently:
            if isinstance(message, RequestReplyMessage):
                await self._handle_request_reply(message, ctx)
            elif isinstance(message, BroadcastMessage):
                await self._handle_broadcast(message, ctx)
            elif isinstance(message, ResetMessage):
                await self._handle_reset(message, ctx)
            elif isinstance(message, DeactivateMessage):
                await self._handle_deactivate(message, ctx)
        else:
            future = asyncio.Future[Any]()
            await self._message_queue.put((message, ctx, future))
            await future

    async def _handle_broadcast(self, message: BroadcastMessage, ctx: MessageContext) -> None:
        raise NotImplementedError()

    async def _handle_reset(self, message: ResetMessage, ctx: MessageContext) -> None:
        raise NotImplementedError()

    async def _handle_request_reply(self, message: RequestReplyMessage, ctx: MessageContext) -> None:
        raise NotImplementedError()

    async def _handle_deactivate(self, message: DeactivateMessage, ctx: MessageContext) -> None:
        """Handle a deactivate message."""
        self._enabled = False
        self.logger.info(
            AgentEvent(
                f"{self.metadata['type']} (deactivated)",
                "",
            )
        )

    async def on_unhandled_message(self, message: Any, ctx: MessageContext) -> None:
        """Drop the message, with a log."""
        # self.logger.info(
        #     AgentEvent(
        #         f"{self.metadata['type']} (unhandled message)",
        #         f"Unhandled message type: {type(message)}",
        #     )
        # )
        pass
